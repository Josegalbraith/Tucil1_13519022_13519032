{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan informasi WHO, stroke merupakan penyebab kematian terbanyak nomor 2 di dunia dan menjadi penyebab dari 11% total kematian. [Dataset](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset) ini digunakan untuk memprediksi apakah seorang pasien memiliki kemungkinan besar untuk terkena stroke berdasarkan informasi tentang pasien seperti jenis kelamin, umur, penyakit dan status merokok.\n",
    "\n",
    "Tujuan eksperimen:\n",
    "1.   Peserta memahami rangkaian proses analitika data menggunakan pendekatan pembelajaran mesin. \n",
    "2.   Peserta memahami bahwa proses pengembangan model pembelajaran mesin juga ditentukan dari kualitas data, penanganan data, dan penentuan algoritma serta hiperparameternya; tidak cukup hanya dengan memastikan implementasi algoritma berjalan tanpa kesalahan.\n",
    "3. Peserta mampu menginterpretasikan hasil dari evaluasi model dalam proses analitika menggunakan pendekatan pembelajaran mesin.\n",
    "\n",
    "Praktikum dilaksanakan secara berkelompok, dengan 1 kelompok terdiri atas 2 mahasiswa. Soal praktikum terdapat di bagian bawah berkas ini. Harap diperhatikan bahwa terdapat berkas yang harus dikumpulkan sebelum waktu praktikum selesai (4 April 2022 11.00 WIB) dan berkas yang dikumpulkan setelah waktu praktikum selesai (4 April 2022 23.59 WIB). Untuk detil deliverables dan soal, dapat dilihat pada bagian bawah notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
    "X = data.drop(columns=\"stroke\")\n",
    "y = data[\"stroke\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=123)\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_val = pd.concat([X_val, y_val], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soal Eksperimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disediakan data yang sudah dibagi menjadi data train (df_train), validasi (df_val), dan test (df_test). Lakukanlah:\n",
    "1. Buatlah baseline dengan menggunakan model Logistic Regression\n",
    "2. Lakukan analisa data terkait:\n",
    "- Duplicate value\n",
    "- Missing value\n",
    "- Outlier\n",
    "- Balance of data\n",
    "3. Jelaskan bagaimana kalian akan menangani permasalahan yang disebutkan pada poin 2\n",
    "4. Sebutkan dan jelaskan alasan dari teknik encoding yang akan kalian gunakan terhadap data tersebut\n",
    "5. Buatlah desain eksperimen dengan menentukan hal berikut:\n",
    "- Tujuan eksperimen\n",
    "- Dependent dan Independent variabel\n",
    "- Strategi eksperimen\n",
    "- Skema validasi\n",
    "6. Implementasikan strategi eksperimen dan skema validasi yang sudah kalian buat\n",
    "7. Berdasarkan hasil prediksi yang kalian hasilkan, buatlah kesimpulan analisis karakteristik pasien yang terkena stroke\n",
    "\n",
    "Poin 1 - 5 dikerjakan saat praktikum berlangsung (pukul 09.00 WIB - 11.00 WIB)\n",
    "Poin 6 - 7 dikerjakan saat setelah praktikum berlangsung (pukul 11.00 WIB - 23.59 WIB)\n",
    "\n",
    "Jika terdapat perubahan jawaban pada poin 1 - 5 (semisal perbedaan cara melakukan handling missing value), dapat dijelaskan pada laporan mengenai jawaban sebelum, jawaban sesudah, dan alasan merubah jawaban tersebut (semisal menemukan suatu hal menarik pada data, sehingga missing value dapat dihandle dengan metode yang lebih bagus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Ubah categorical \n",
    "lb = LabelEncoder()\n",
    "df_train['gender'] = lb.fit_transform(df_train['gender'] ) \n",
    "df_val['gender'] = lb.fit_transform(df_val['gender'] ) \n",
    "df_test['gender'] = lb.fit_transform(df_test['gender'] ) \n",
    "\n",
    "df_train['ever_married'] = lb.fit_transform(df_train['ever_married'] ) \n",
    "df_val['ever_married'] = lb.fit_transform(df_val['ever_married'] ) \n",
    "df_test['ever_married'] = lb.fit_transform(df_test['ever_married'] ) \n",
    "\n",
    "df_train['work_type'] = lb.fit_transform(df_train['work_type'] ) \n",
    "df_val['work_type'] = lb.fit_transform(df_val['work_type'] ) \n",
    "df_test['work_type'] = lb.fit_transform(df_test['work_type'] ) \n",
    "\n",
    "df_train['work_type'] = lb.fit_transform(df_train['work_type'] ) \n",
    "df_val['work_type'] = lb.fit_transform(df_val['work_type'] ) \n",
    "df_test['work_type'] = lb.fit_transform(df_test['work_type'] ) \n",
    "\n",
    "df_train['Residence_type'] = lb.fit_transform(df_train['Residence_type'] ) \n",
    "df_val['Residence_type'] = lb.fit_transform(df_val['Residence_type']) \n",
    "df_test['Residence_type'] = lb.fit_transform(df_test['Residence_type']) \n",
    "\n",
    "df_train['smoking_status'] = lb.fit_transform(df_train['smoking_status'] ) \n",
    "df_val['smoking_status'] = lb.fit_transform(df_val['smoking_status']) \n",
    "df_test['smoking_status'] = lb.fit_transform(df_test['smoking_status']) \n",
    "\n",
    "\n",
    "df_train['gender'] = lb.fit_transform(df_train['gender'] ) \n",
    "df_val['gender'] = lb.fit_transform(df_val['gender'] ) \n",
    "df_test['gender'] = lb.fit_transform(df_test['gender'] ) \n",
    "\n",
    "df_train['gender'].fillna(df_train['gender'].mean(), inplace= True)\n",
    "df_test['gender'].fillna(df_test['gender'].mean(), inplace= True)\n",
    "df_val['gender'].fillna(df_val['gender'].mean(), inplace= True)\n",
    "\n",
    "df_train['bmi'].fillna(df_train['bmi'].mean(), inplace= True)\n",
    "df_test['bmi'].fillna(df_test['bmi'].mean(), inplace= True)\n",
    "df_val['bmi'].fillna(df_val['bmi'].mean(), inplace= True)\n",
    "model = LogisticRegression()\n",
    "X_train = df_train.drop(columns=\"stroke\")\n",
    "y_train = df_train[\"stroke\"].copy()\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean = df_train.duplicated().any()\n",
    "boolean = df_train.duplicated(subset=['id','gender','age','hypertension','heart_disease','ever_married','work_type','Residence_type','avg_glucose_level','bmi','smoking_status','stroke']).any()\n",
    "boolean\n",
    "#Tidak ada duplicate Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lakukan analisa data terkait:\n",
    "- Duplicate value\n",
    "- Missing value\n",
    "- Outlier\n",
    "- Balance of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate value: \n",
      " Data train duplicate value:  0\n",
      " Data validasi duplicate value:  0\n",
      " Data test duplicate value:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Duplicate value: \")\n",
    "print(\" Data train duplicate value: \", df_train.duplicated().sum())\n",
    "print(\" Data validasi duplicate value: \", df_val.duplicated().sum())\n",
    "print(\" Data test duplicate value: \", df_test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value: \n",
      " Data train missing value:  id                   0\n",
      "gender               0\n",
      "age                  0\n",
      "hypertension         0\n",
      "heart_disease        0\n",
      "ever_married         0\n",
      "work_type            0\n",
      "Residence_type       0\n",
      "avg_glucose_level    0\n",
      "bmi                  0\n",
      "smoking_status       0\n",
      "stroke               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing value: \")\n",
    "print(\" Data train missing value: \", df_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data validasi missing value:  id                   0\n",
      "gender               0\n",
      "age                  0\n",
      "hypertension         0\n",
      "heart_disease        0\n",
      "ever_married         0\n",
      "work_type            0\n",
      "Residence_type       0\n",
      "avg_glucose_level    0\n",
      "bmi                  0\n",
      "smoking_status       0\n",
      "stroke               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\" Data validasi missing value: \", df_val.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data test missing value:  id                   0\n",
      "gender               0\n",
      "age                  0\n",
      "hypertension         0\n",
      "heart_disease        0\n",
      "ever_married         0\n",
      "work_type            0\n",
      "Residence_type       0\n",
      "avg_glucose_level    0\n",
      "bmi                  0\n",
      "smoking_status       0\n",
      "stroke               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\" Data test missing value: \", df_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier: \n",
      "Outlier Data Train: \n",
      "id: 0\n",
      "gender: 0\n",
      "age: 0\n",
      "hypertension: 6244\n",
      "heart_disease: 6355\n",
      "ever_married: 0\n",
      "work_type: 427\n",
      "Residence_type: 0\n",
      "avg_glucose_level: 414\n",
      "bmi: 91\n",
      "smoking_status: 0\n",
      "stroke: 6379\n"
     ]
    }
   ],
   "source": [
    "print(\"Outlier: \")\n",
    "print(\"Outlier Data Train: \")\n",
    "def outlier_data_train(column):\n",
    "    Q1 = np.percentile(df_train[column], 25,\n",
    "                    interpolation = 'midpoint')\n",
    "    \n",
    "    Q3 = np.percentile(df_train[column], 75,\n",
    "                   interpolation = 'midpoint')\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    upper = df_train[column] >= (Q3+1.5*IQR)\n",
    "    count_upper = len(np.where(upper)[0])\n",
    "\n",
    "    lower = df_train[column] <= (Q1-1.5*IQR)\n",
    "    count_lower = len(np.where(lower)[0])\n",
    "\n",
    "    count_outlier = count_upper + count_lower\n",
    "    return(count_outlier)\n",
    "\n",
    "for col in df_train.columns:\n",
    "    print(col + \": \" + str(outlier_data_train(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier Data Validasi: \n",
      "id: 0\n",
      "gender: 0\n",
      "age: 0\n",
      "hypertension: 1550\n",
      "heart_disease: 1592\n",
      "ever_married: 0\n",
      "work_type: 107\n",
      "Residence_type: 0\n",
      "avg_glucose_level: 106\n",
      "bmi: 20\n",
      "smoking_status: 0\n",
      "stroke: 1598\n"
     ]
    }
   ],
   "source": [
    "print(\"Outlier Data Validasi: \")\n",
    "def outlier_data_validasi(column):\n",
    "    Q1 = np.percentile(df_val[column], 25,\n",
    "                    interpolation = 'midpoint')\n",
    "    \n",
    "    Q3 = np.percentile(df_val[column], 75,\n",
    "                   interpolation = 'midpoint')\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    upper = df_val[column] >= (Q3+1.5*IQR)\n",
    "    count_upper = len(np.where(upper)[0])\n",
    "\n",
    "    lower = df_val[column] <= (Q1-1.5*IQR)\n",
    "    count_lower = len(np.where(lower)[0])\n",
    "\n",
    "    count_outlier = count_upper + count_lower\n",
    "    return(count_outlier)\n",
    "\n",
    "for col in df_val.columns:\n",
    "    print(col + \": \" + str(outlier_data_validasi(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier Data Test: \n",
      "id: 0\n",
      "gender: 0\n",
      "age: 0\n",
      "hypertension: 1928\n",
      "heart_disease: 1997\n",
      "ever_married: 0\n",
      "work_type: 123\n",
      "Residence_type: 0\n",
      "avg_glucose_level: 101\n",
      "bmi: 19\n",
      "smoking_status: 0\n",
      "stroke: 1994\n"
     ]
    }
   ],
   "source": [
    "print(\"Outlier Data Test: \")\n",
    "def outlier_data_test(column):\n",
    "    Q1 = np.percentile(df_test[column], 25,\n",
    "                    interpolation = 'midpoint')\n",
    "    \n",
    "    Q3 = np.percentile(df_test[column], 75,\n",
    "                   interpolation = 'midpoint')\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    upper = df_test[column] >= (Q3+1.5*IQR)\n",
    "    count_upper = len(np.where(upper)[0])\n",
    "\n",
    "    lower = df_test[column] <= (Q1-1.5*IQR)\n",
    "    count_lower = len(np.where(lower)[0])\n",
    "\n",
    "    count_outlier = count_upper + count_lower\n",
    "    return(count_outlier)\n",
    "\n",
    "for col in df_test.columns:\n",
    "    print(col + \": \" + str(outlier_data_test(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance of Data: \n",
      " Oversampling: \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_euclidean_distances' from 'sklearn.metrics.pairwise' (C:\\Users\\joseg\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7332/2497497733.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRepeatedStratifiedKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# process, as it may not be compiled yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcombine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\combine\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\combine\\_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEditedNearestNeighbours\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\over_sampling\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_adasyn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mADASYN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_random_over_sampler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_smote\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_smote\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBorderlineSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_smote\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeansSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTENC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeansSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBorderlineSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\cluster.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMiniBatchKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_spectral\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspectral_clustering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpectralClustering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_mean_shift\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_shift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMeanShift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimate_bandwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_bin_seeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_affinity_propagation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maffinity_propagation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAffinityPropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_spectral.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspectral_embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_kmeans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mk_means\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mClusterMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_euclidean_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstable_cumsum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mthreadpool_limits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_euclidean_distances' from 'sklearn.metrics.pairwise' (C:\\Users\\joseg\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py)"
     ]
    }
   ],
   "source": [
    "print(\"Balance of Data: \")\n",
    "print(\" Oversampling: \")\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# define pipeline\n",
    "steps = [('over', RandomOverSampler()), ('model', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, scoring='f1_micro', cv=cv, n_jobs=-1)\n",
    "score = mean(scores)\n",
    "print(' F1 Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Undersampling: \n",
      " F1 Score: 0.687\n"
     ]
    }
   ],
   "source": [
    "print(\" Undersampling: \")\n",
    "# define pipeline\n",
    "steps = [('under', RandomUnderSampler()), ('model', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, scoring='f1_micro', cv=cv, n_jobs=-1)\n",
    "score = mean(scores)\n",
    "print(' F1 Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Jelaskan bagaimana kalian akan menangani permasalahan yang disebutkan pada poin 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pada data duplicate kami tidak perlu menangani apa-apa dikarenakan tidak terdapat data duplicate pada dataset\n",
    "2. Pada missing value kami menanganinya dengan mengisi missing value dengan nilai mean dari instance-instance yang terdapat pada feature tersebut\n",
    "3. Pada outlier kami mendeteksinya dengan menggunakan metode IQR (Inter Quartile Range) dan akan menghapus outlier-outlier tersebut\n",
    "4. Pada Balance of Data kami tidak perlu menangani apa-apa dikarenakan data sudah cukup balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0\n",
       "gender                 0\n",
       "age                    0\n",
       "hypertension           0\n",
       "heart_disease          0\n",
       "ever_married           0\n",
       "work_type              0\n",
       "Residence_type         0\n",
       "avg_glucose_level      0\n",
       "bmi                  130\n",
       "smoking_status         0\n",
       "stroke                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "gender               0\n",
       "age                  0\n",
       "hypertension         0\n",
       "heart_disease        0\n",
       "ever_married         0\n",
       "work_type            0\n",
       "Residence_type       0\n",
       "avg_glucose_level    0\n",
       "bmi                  0\n",
       "smoking_status       0\n",
       "stroke               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Value Handling\n",
    "df_train['bmi'].fillna(df_train['bmi'].mean(), inplace= True)\n",
    "df_test['bmi'].fillna(df_test['bmi'].mean(), inplace= True)\n",
    "df_val['bmi'].fillna(df_val['bmi'].mean(), inplace= True)\n",
    "\n",
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sebutkan dan jelaskan alasan dari teknik encoding yang akan kalian gunakan terhadap data tersebut\n",
    "Encoding yang digunakan adalah dengan menggunakan LabelEncoder untuk mengubah kategorikal atribut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Buatlah desain eksperimen dengan menentukan hal berikut:\n",
    "- Tujuan eksperimen\n",
    "- Dependent dan Independent variabel\n",
    "- Strategi eksperimen\n",
    "- Skema validasi\n",
    "\n",
    "Tujuan Eksperimen :\n",
    "1.   Peserta memahami rangkaian proses analitika data menggunakan pendekatan pembelajaran mesin. \n",
    "2.   Peserta memahami bahwa proses pengembangan model pembelajaran mesin juga ditentukan dari kualitas data, penanganan data, dan penentuan algoritma serta hiperparameternya; tidak cukup hanya dengan memastikan implementasi algoritma berjalan tanpa kesalahan.\n",
    "3. Peserta mampu menginterpretasikan hasil dari evaluasi model dalam proses analitika menggunakan pendekatan pembelajaran mesin.\n",
    "\n",
    "Dependent Variabel : 'stroke\n",
    "Independent Variable : 'id','gender','age','hypertension','heart_disease','ever_married','work_type','Residence_type','avg_glucose_level','bmi','smoking_status','stroke'\n",
    "\n",
    "Strategi Eksperimen : Grid Search\n",
    "\n",
    "Skema Validasi : K - Fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Implementasi Strategi Eksperimen dan Skema Validasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1600 candidates, totalling 4800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseg\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ...        nan 0.95076453 0.95076453]\n",
      "  warnings.warn(\n",
      "C:\\Users\\joseg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "logModel = LogisticRegression()\n",
    "\n",
    "param_grid = [\n",
    "    {'penalty' : ['11','12','elasticnet','none'],\n",
    "    'C' : np.logspace(-4,4,20),\n",
    "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "    'max_iter' : [100,1000,2500,5000]\n",
    "    }\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(logModel, param_grid=param_grid, cv = 3, verbose=True, n_jobs=-1)\n",
    "\n",
    "best_clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, penalty='none')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - : 0.950\n"
     ]
    }
   ],
   "source": [
    "#Check Accuracy\n",
    "print(f'Accuracy - : {best_clf.score(X_train,y_train):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2555 2556 2557 ... 5107 5108 5109] TEST: [   0    1    2 ... 2552 2553 2554]\n",
      "TRAIN: [   0    1    2 ... 2552 2553 2554] TEST: [2555 2556 2557 ... 5107 5108 5109]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=2)\n",
    "for train_index, test_index in kf.split(X):\n",
    "     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Berdasarkan hasil prediksi yang kalian hasilkan, buatlah kesimpulan analisis karakteristik pasien yang terkena stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.17621241e-05 -2.16705433e-03  4.44544031e-02  1.93261650e-03\n",
      "   1.68858375e-03 -1.07100679e-03 -1.60420874e-02 -2.61508086e-03\n",
      "   2.40582032e-03 -1.49977854e-01 -6.90380742e-03]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joseg\\AppData\\Local\\Temp/ipykernel_7332/825289690.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['ever_married'] = lb.fit_transform(X_train['ever_married'] )\n",
      "C:\\Users\\joseg\\AppData\\Local\\Temp/ipykernel_7332/825289690.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['work_type'] = lb.fit_transform(X_train['work_type'] )\n",
      "C:\\Users\\joseg\\AppData\\Local\\Temp/ipykernel_7332/825289690.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['work_type'] = lb.fit_transform(X_train['work_type'] )\n",
      "C:\\Users\\joseg\\AppData\\Local\\Temp/ipykernel_7332/825289690.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['Residence_type'] = lb.fit_transform(X_train['Residence_type'] )\n",
      "C:\\Users\\joseg\\AppData\\Local\\Temp/ipykernel_7332/825289690.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['smoking_status'] = lb.fit_transform(X_train['smoking_status'] )\n",
      "C:\\Users\\joseg\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6392: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>202.21</td>\n",
       "      <td>28.914618</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.400000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease  ever_married  work_type  \\\n",
       "0   9046       1  67.0             0              1             1          2   \n",
       "1  51676       0  61.0             0              0             1          3   \n",
       "2  31112       1  80.0             0              1             1          2   \n",
       "3  60182       0  49.0             0              0             1          2   \n",
       "4   1665       0  79.0             1              0             1          3   \n",
       "\n",
       "   Residence_type  avg_glucose_level        bmi  smoking_status  \n",
       "0               1             228.69  36.600000               1  \n",
       "1               0             202.21  28.914618               2  \n",
       "2               0             105.92  32.500000               2  \n",
       "3               1             171.23  34.400000               3  \n",
       "4               0             174.12  24.000000               2  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "X_train['ever_married'] = lb.fit_transform(X_train['ever_married'] ) \n",
    "\n",
    "X_train['work_type'] = lb.fit_transform(X_train['work_type'] ) \n",
    "\n",
    "X_train['work_type'] = lb.fit_transform(X_train['work_type'] ) \n",
    "\n",
    "X_train['Residence_type'] = lb.fit_transform(X_train['Residence_type'] ) \n",
    "\n",
    "X_train['smoking_status'] = lb.fit_transform(X_train['smoking_status'] )\n",
    "\n",
    "X_train['bmi'].fillna(df_train['bmi'].mean(), inplace= True)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "print(model.coef_)\n",
    "X_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan hasil dari coef_ maka karakteristik dari orang yang mengalami stroke adalah sudah berumur, mengalami hypertension, mengalami heart_disease, memiliki avg_glucose_level yang tinggi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jelaskan pembagian tugas/ kerja antar anggota kelompok saat eksperimen,  pada sel ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jose Galbraith H.\n",
    "1. Transform categorical value\n",
    "2. Mengisi NaN value dengan mean\n",
    "3. Mengerjakan Desain Eksperimen\n",
    "\n",
    "Muhammad Fahkry Malta\n",
    "1. Menganalisa Duplicate Value, Missing Value, Outlier, dan Balance of Data\n",
    "2. Membuat hasil analisis dari analisa diatas (nomor 3)\n",
    "3. Membuat laporan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Notebook dengan nama file PraktikumIF3270_M1_NIM1_NIM2.ipynb untuk poin 1 - 5.\n",
    "2. Notebook dengan nama file PraktikumIF3270_M2_NIM1_NIM2.ipynb yang merupakan kelanjutan dari notebook poin 1, dengan tambahan hasil poin 6 dan 7.\n",
    "3. Laporan dengan nama file PraktikumIF3270_NIM1_NIM2.pdf dengan isi sebagai berikut:\n",
    "- Hasil analisa terhadap data, penanganan yang dilakukan serta justifikasi teknik-teknik yang dipilih\n",
    "- Perubahan yang dilakukan pada jawaban poin 1 - 5 jika ada\n",
    "- Desain eksperimen\n",
    "- Hasil eksperimen\n",
    "- Analisis dan kesimpulan\n",
    "- Pembagian Tugas / Kerja antar anggota kelompok\n",
    "\n",
    "Deadline pengumpulan:\n",
    "- Deliverables poin 1 dikumpulkan sebelum <b>pukul 11.00 WIB</b>, Senin 4 April 2022\n",
    "- Deliverables poin 2 dikumpulkan sebelum <b>pukul 23.59 WIB</b>, Senin 4 April 2022\n",
    "- Deliverables poin 3 dikumpulkan sebelum <b>pukul 23.59 WIB</b>, Senin 4 April 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
